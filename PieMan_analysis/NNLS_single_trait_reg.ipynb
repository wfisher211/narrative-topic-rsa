{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Trait Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, re, csv\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import statsmodels.api as sm\n",
    "from nilearn.image import resample_to_img\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_1samp\n",
    "from nilearn import plotting, datasets\n",
    "import scipy.stats as stats\n",
    "from scipy.optimize import nnls   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voxel size (mm): (3.0, 3.0, 4.0)\n",
      "TR (s): 1.0\n",
      "Shape (65, 77, 49, 160)\n"
     ]
    }
   ],
   "source": [
    "# Load the BOLD cleaned image\n",
    "bold_img = nib.load('/Volumes/Passport/fmriprep/derivatives/pieMan_cleaned/sub-002/func/sub-002_task-pieman_run-1_cleaned_desc-masked_bold.nii.gz')\n",
    "\n",
    "# Print voxel size (spatial resolution) and TR (temporal resolution)\n",
    "zooms = bold_img.header.get_zooms()\n",
    "print(f\"Voxel size (mm): {zooms[:3]}\")\n",
    "print(f\"TR (s): {zooms[3]}\")\n",
    "print(f\"Shape {bold_img.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET MAIN HYPERPARAMETERS\n",
    "# TRAIT_LABEL = \"Contemplating\"  \n",
    "ALL_TRAIT_LABELS = [\n",
    "    \"Open-minded\",\"feeling Affectionate\",\"Attentive\",\"Assertive\",\n",
    "    \"feeling Gloomy\",\"feeling Peaceful\",\"Agreeable\",\"Judging\",\n",
    "    \"feeling Angry\",\"feeling Bewildered\",\"Impulsive\",\n",
    "    \"Self-disciplined\",\"Contemplating\"\n",
    "]\n",
    "ALL_TRAIT_SAVE_STRS = [t.replace(\" \",\"_\").replace(\"-\",\"_\")\n",
    "                       for t in ALL_TRAIT_LABELS]\n",
    "# Our 13 trait labels \n",
    "# [\"Open-minded\", \"feeling Affectionate\", \"Attentive\", \"Assertive\", \"feeling Gloomy\", \"feeling Peaceful\", \"Agreeable\", \"Judging\", \"feeling Angry\", \"feeling Bewildered\", \"Impulsive\", \"Self-disciplined\", \"Contemplating\"]\n",
    "\n",
    "#TRAIT_LABEL_SAVE_STRING = TRAIT_LABEL.replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "STIMULUS_LABEL_SAVE_STRING = \"pieman\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────\n",
    "# 0) PATHS & I/O\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "root_dir  = \"/Volumes/Passport/fmriprep\"          # ←  same as in cleaning script\n",
    "deriv_dir = os.path.join(root_dir, \"derivatives\") #   (don’t hard-code “subjects” yet)\n",
    "\n",
    "\n",
    "\n",
    "# output from your behaviour-model RSA\n",
    "for trait_long, trait_save in zip(ALL_TRAIT_LABELS, ALL_TRAIT_SAVE_STRS):\n",
    "    rdm_path = os.path.join(\n",
    "        deriv_dir, \"RDMs_behavior\",\n",
    "        f\"{STIMULUS_LABEL_SAVE_STRING}_{trait_save}_RDM.npy\"\n",
    "    )\n",
    "    model_rdm = np.load(rdm_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────\n",
    "# 1) SUBJECT / RUN FILTERS  (copy-paste verbatim)  ─────────────\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "exclude_subs = {\n",
    "    \"sub-001\",\"sub-021\",\"sub-022\",\"sub-038\",\"sub-056\",\"sub-068\",\"sub-069\"\n",
    "}\n",
    "exclude_sub_runs = {\n",
    "    (\"sub-002\",\"2\"),(\"sub-003\",\"2\"),(\"sub-004\",\"2\"),(\"sub-005\",\"2\"),(\"sub-006\",\"2\"),\n",
    "    (\"sub-008\",\"2\"),(\"sub-010\",\"2\"),(\"sub-011\",\"2\"),(\"sub-012\",\"2\"),(\"sub-013\",\"2\"),\n",
    "    (\"sub-014\",\"2\"),(\"sub-015\",\"2\"),(\"sub-016\",\"2\")\n",
    "}\n",
    "target_subject = None     # e.g. \"sub-002\" to run a single person\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subjects to process → sub-002, sub-003, sub-004, sub-005, sub-006, sub-007, sub-008, sub-009, sub-010, sub-011, sub-012, sub-013, sub-014, sub-015, sub-016, sub-017, sub-018, sub-019, sub-020, sub-023, sub-024, sub-025, sub-026, sub-027, sub-028, sub-029, sub-030, sub-031, sub-032, sub-033, sub-034, sub-035, sub-036, sub-037, sub-039, sub-040, sub-041, sub-042, sub-043, sub-044, sub-045, sub-046, sub-047, sub-048, sub-049, sub-050, sub-051, sub-052, sub-053, sub-054, sub-055, sub-057, sub-058, sub-059, sub-060, sub-061, sub-062, sub-063, sub-064, sub-065, sub-066, sub-067, sub-070, sub-071, sub-072, sub-073, sub-074, sub-075, sub-076, sub-077, sub-078, sub-079, sub-080, sub-081, sub-082\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# 2)  BUILD SUBJECT LIST  (from cleaned derivatives)  ------------\n",
    "# ----------------------------------------------------------------\n",
    "cleaned_root = os.path.join(deriv_dir, f\"{STIMULUS_LABEL_SAVE_STRING}_cleaned\")\n",
    "all_subs     = sorted(\n",
    "    d for d in os.listdir(cleaned_root) if d.startswith(\"sub-\")\n",
    ")\n",
    "if target_subject:\n",
    "    if target_subject not in all_subs:\n",
    "        raise ValueError(f\"{target_subject} not found in {cleaned_root}\")\n",
    "    subjects = [target_subject]\n",
    "else:\n",
    "    subjects = [s for s in all_subs if s not in exclude_subs]\n",
    "\n",
    "print(\"Subjects to process →\", \", \".join(subjects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────\n",
    "# 3) FETCH SCHAEFER ATLAS  ─────────────────────────────────────\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "\n",
    "# Schaefer parcel/atlas parameters\n",
    "n_rois = 200\n",
    "yeo_networks = 17\n",
    "resolution_mm = 2                   # resolution of your Schaefer atlas (double check!)\n",
    "\n",
    "schaefer    = datasets.fetch_atlas_schaefer_2018(\n",
    "                 n_rois=n_rois,\n",
    "                 yeo_networks=yeo_networks,\n",
    "                 resolution_mm=resolution_mm\n",
    "             )\n",
    "atlas_img   = nib.load(schaefer['maps'])  # default 2mm MNI - but our images 3x3x4 (Pieman and others) OR 2.5^3 (ie., Black and Forgot)\n",
    "\n",
    "atlas_resampled = resample_to_img(atlas_img, bold_img, interpolation='nearest')\n",
    "atlas_data     = atlas_resampled.get_fdata()\n",
    "\n",
    "\n",
    "\n",
    "# Change Schaeffer Labels so 0 is whole brain and 1 corresponds to 1st ROI\n",
    "labels = schaefer['labels']\n",
    "# change to string and remove excess\n",
    "labels = [l.replace(b'17Networks_', b'').decode('utf-8') for l in labels]\n",
    "# Prepend background label\n",
    "labels = np.insert(labels, 0, \"Background\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────\n",
    "# 4) Define NNLS function  ─────────────────────────────────────\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "def rsa_nnls(neural_rdm: np.ndarray, model_rdm: np.ndarray):\n",
    "    \"\"\"\n",
    "    Non-Negative Least-Squares RSA.\n",
    "    Returns\n",
    "    -------\n",
    "    beta : float   – coefficient for the model RDM (≥ 0) \n",
    "    r2   : float   – coefficient of determination for the NNLS fit\n",
    "    \"\"\"\n",
    "    # ----------------------------------------------------------\n",
    "    # 1) VECTORISE LOWER-TRIANGLE (k = -1 keeps diagonal out)\n",
    "    # ----------------------------------------------------------\n",
    "    i_low = np.tril_indices(160, k=-1)\n",
    "    y = neural_rdm[i_low]                   # dependent variable, shape (N,)\n",
    "    X = model_rdm[i_low][:, None]           # predictor,      shape (N,1)\n",
    "\n",
    "    # add a constant column so the fit can pick up any baseline offset;\n",
    "    # both columns are subject to the non-negativity constraint.\n",
    "    X = np.column_stack([np.ones_like(y), X])   # shape (N,2)\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # 2) NNLS FIT\n",
    "    # ----------------------------------------------------------\n",
    "    coef, _ = nnls(X, y)            # coef[0] = intercept, coef[1] = beta (≥0)\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # 3) GOODNESS-OF-FIT (pseudo-R²)\n",
    "    # ----------------------------------------------------------\n",
    "    y_hat = X @ coef\n",
    "    ss_res = np.sum((y - y_hat) ** 2)\n",
    "    ss_tot = np.sum((y - y.mean()) ** 2)\n",
    "    r2     = 1 - ss_res / ss_tot if ss_tot > 0 else 0.0\n",
    "\n",
    "    beta = float(coef[1])\n",
    "    return beta, float(r2)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 5) Single Trait per-subject function  (adds run-skip check)  ─────────\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "def run_parcel_rsa_for_subject(sub: str,\n",
    "        trait_save: str,\n",
    "        model_rdm: np.ndarray):\n",
    "    func_dir   = os.path.join(cleaned_root, sub, \"func\")\n",
    "    run_pat    = os.path.join(func_dir,\n",
    "                  f\"{sub}_task-{STIMULUS_LABEL_SAVE_STRING}_run-*_*cleaned_desc-masked_bold.nii.gz\")\n",
    "    single_pat = os.path.join(func_dir,\n",
    "                  f\"{sub}_task-{STIMULUS_LABEL_SAVE_STRING}_cleaned_desc-masked_bold.nii.gz\")\n",
    "    bold_files = sorted(glob.glob(run_pat)) + sorted(glob.glob(single_pat))\n",
    "    if not bold_files:\n",
    "        print(f\"⏩ {sub}: no cleaned runs\")\n",
    "        return\n",
    "\n",
    "    rows = []\n",
    "    for bf in bold_files:\n",
    "        m   = re.search(r\"_run-(\\d+)_\", os.path.basename(bf))\n",
    "        run = m.group(1) if m else None\n",
    "        if run and (sub, run) in exclude_sub_runs:\n",
    "            print(f\"   • skipping {sub} run-{run} (in exclusion list)\")\n",
    "            continue\n",
    "\n",
    "        bold_img  = nib.load(bf)\n",
    "        bold_data = bold_img.get_fdata()\n",
    "\n",
    "        atlas_resampled = resample_to_img(atlas_img, bold_img, interpolation=\"nearest\")\n",
    "        atlas_data      = atlas_resampled.get_fdata()\n",
    "\n",
    "        for parcel_id in range(1, n_rois + 1):\n",
    "            mask = atlas_data == parcel_id\n",
    "            if not mask.any(): continue\n",
    "            rdm   = 1 - np.corrcoef(bold_data[mask, :].T).astype(np.float32)\n",
    "            beta, r2 = rsa_nnls(rdm, model_rdm)  \n",
    "            parcel_label = labels[parcel_id]\n",
    "            rows.append([sub, run or \"NA\", parcel_id, parcel_label, beta, r2])\n",
    "            \n",
    "    # ---- write one CSV per subject ----\n",
    "    out_dir  = os.path.join(deriv_dir, \"RSA_stats\")\n",
    "\n",
    "    # add the stimulus‐specific subfolder (e.g. \"pieman\")\n",
    "    stim_folder = os.path.join(out_dir, STIMULUS_LABEL_SAVE_STRING)\n",
    "    os.makedirs(stim_folder, exist_ok=True)\n",
    "\n",
    "    # add the trait‐specific subfolder (e.g. \"feeling_Affectionate\")\n",
    "    trait_folder = os.path.join(stim_folder, trait_save)\n",
    "    os.makedirs(trait_folder, exist_ok=True)\n",
    "\n",
    "    out_csv = os.path.join(trait_folder, f\"{sub}_{STIMULUS_LABEL_SAVE_STRING}_{trait_save}_parcel_RSA_NNLS.csv\")\n",
    "    with open(out_csv, \"w\", newline=\"\") as f:\n",
    "        csv.writer(f).writerows([[\"subject\",\"run\",\"parcel_num\",\"parcel_label\",\"beta\",\"r2\"]] + rows)\n",
    "    print(f\"✅ {sub}: {len(rows)} rows → {out_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "── TRAIT: Open-minded ──\n",
      "✅ sub-002: 200 rows → /Volumes/Passport/fmriprep/derivatives/RSA_stats/pieman/Open_minded/sub-002_pieman_Open_minded_parcel_RSA_NNLS.csv\n",
      "✅ sub-003: 200 rows → /Volumes/Passport/fmriprep/derivatives/RSA_stats/pieman/Open_minded/sub-003_pieman_Open_minded_parcel_RSA_NNLS.csv\n",
      "✅ sub-004: 200 rows → /Volumes/Passport/fmriprep/derivatives/RSA_stats/pieman/Open_minded/sub-004_pieman_Open_minded_parcel_RSA_NNLS.csv\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# 2) run every subject for this trait\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sub \u001b[38;5;129;01min\u001b[39;00m subjects:\n\u001b[0;32m---> 16\u001b[0m         run_parcel_rsa_for_subject(sub, trait_save, model_rdm)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mALL DONE 🎉\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 66\u001b[0m, in \u001b[0;36mrun_parcel_rsa_for_subject\u001b[0;34m(sub, trait_save, model_rdm)\u001b[0m\n\u001b[1;32m     63\u001b[0m bold_img  \u001b[38;5;241m=\u001b[39m nib\u001b[38;5;241m.\u001b[39mload(bf)\n\u001b[1;32m     64\u001b[0m bold_data \u001b[38;5;241m=\u001b[39m bold_img\u001b[38;5;241m.\u001b[39mget_fdata()\n\u001b[0;32m---> 66\u001b[0m atlas_resampled \u001b[38;5;241m=\u001b[39m resample_to_img(atlas_img, bold_img, interpolation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnearest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     67\u001b[0m atlas_data      \u001b[38;5;241m=\u001b[39m atlas_resampled\u001b[38;5;241m.\u001b[39mget_fdata()\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m parcel_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, n_rois \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nilearn/image/resampling.py:748\u001b[0m, in \u001b[0;36mresample_to_img\u001b[0;34m(source_img, target_img, interpolation, copy, order, clip, fill_value, force_resample)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresample_to_img\u001b[39m(\n\u001b[1;32m    688\u001b[0m     source_img,\n\u001b[1;32m    689\u001b[0m     target_img,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    695\u001b[0m     force_resample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    696\u001b[0m ):\n\u001b[1;32m    697\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Resample a Niimg-like source image on a target Niimg-like image.\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \n\u001b[1;32m    699\u001b[0m \u001b[38;5;124;03m    No registration is performed: the image should already be aligned.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    746\u001b[0m \n\u001b[1;32m    747\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 748\u001b[0m     target \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mcheck_niimg(target_img)\n\u001b[1;32m    749\u001b[0m     target_shape \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    751\u001b[0m     \u001b[38;5;66;03m# When target shape is greater than 3, we reduce to 3, to be compatible\u001b[39;00m\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;66;03m# with underlying call to resample_img\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nilearn/_utils/niimg_conversions.py:315\u001b[0m, in \u001b[0;36mcheck_niimg\u001b[0;34m(niimg, ensure_ndim, atleast_4d, dtype, return_iterator, wildcards)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ni\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mconcat_imgs(\n\u001b[1;32m    311\u001b[0m         niimg, ensure_ndim\u001b[38;5;241m=\u001b[39mensure_ndim, dtype\u001b[38;5;241m=\u001b[39mdtype\n\u001b[1;32m    312\u001b[0m     )\n\u001b[1;32m    314\u001b[0m \u001b[38;5;66;03m# Otherwise, it should be a filename or a SpatialImage, we load it\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m niimg \u001b[38;5;241m=\u001b[39m load_niimg(niimg, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_ndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(niimg\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m niimg\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;66;03m# \"squeeze\" the image.\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     data \u001b[38;5;241m=\u001b[39m safe_get_data(niimg)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nilearn/_utils/niimg.py:135\u001b[0m, in \u001b[0;36mload_niimg\u001b[0;34m(niimg, dtype)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(niimg, nibabel\u001b[38;5;241m.\u001b[39mspatialimages\u001b[38;5;241m.\u001b[39mSpatialImage):\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData given cannot be loaded because it is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m not compatible with nibabel format:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;241m+\u001b[39m _repr_niimgs(niimg, shorten\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    133\u001b[0m     )\n\u001b[0;32m--> 135\u001b[0m dtype \u001b[38;5;241m=\u001b[39m _get_target_dtype(_get_data(niimg)\u001b[38;5;241m.\u001b[39mdtype, dtype)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# Copyheader and set dtype in header if header exists\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m niimg\u001b[38;5;241m.\u001b[39mheader \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nilearn/_utils/niimg.py:25\u001b[0m, in \u001b[0;36m_get_data\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m img\u001b[38;5;241m.\u001b[39m_data_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39m_data_cache\n\u001b[0;32m---> 25\u001b[0m data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(img\u001b[38;5;241m.\u001b[39m_dataobj)\n\u001b[1;32m     26\u001b[0m img\u001b[38;5;241m.\u001b[39m_data_cache \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nibabel/arrayproxy.py:454\u001b[0m, in \u001b[0;36mArrayProxy.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    434\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Read data from file and apply scaling, casting to ``dtype``\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \n\u001b[1;32m    436\u001b[0m \u001b[38;5;124;03m    If ``dtype`` is unspecified, the dtype of the returned array is the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;124;03m        Scaled image data with type `dtype`.\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 454\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_scaled(dtype\u001b[38;5;241m=\u001b[39mdtype, slicer\u001b[38;5;241m=\u001b[39m())\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    456\u001b[0m         arr \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nibabel/arrayproxy.py:421\u001b[0m, in \u001b[0;36mArrayProxy._get_scaled\u001b[0;34m(self, dtype, slicer)\u001b[0m\n\u001b[1;32m    419\u001b[0m     scl_inter \u001b[38;5;241m=\u001b[39m scl_inter\u001b[38;5;241m.\u001b[39mastype(use_dtype)\n\u001b[1;32m    420\u001b[0m \u001b[38;5;66;03m# Read array and upcast as necessary for big slopes, intercepts\u001b[39;00m\n\u001b[0;32m--> 421\u001b[0m scaled \u001b[38;5;241m=\u001b[39m apply_read_scaling(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_unscaled(slicer\u001b[38;5;241m=\u001b[39mslicer), scl_slope, scl_inter)\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    423\u001b[0m     scaled \u001b[38;5;241m=\u001b[39m scaled\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mpromote_types(scaled\u001b[38;5;241m.\u001b[39mdtype, dtype), copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nibabel/arrayproxy.py:391\u001b[0m, in \u001b[0;36mArrayProxy._get_unscaled\u001b[0;34m(self, slicer)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m canonical_slicers(slicer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m==\u001b[39m canonical_slicers(\n\u001b[1;32m    388\u001b[0m     (), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    389\u001b[0m ):\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_fileobj() \u001b[38;5;28;01mas\u001b[39;00m fileobj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m--> 391\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m array_from_file(\n\u001b[1;32m    392\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape,\n\u001b[1;32m    393\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dtype,\n\u001b[1;32m    394\u001b[0m             fileobj,\n\u001b[1;32m    395\u001b[0m             offset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_offset,\n\u001b[1;32m    396\u001b[0m             order\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morder,\n\u001b[1;32m    397\u001b[0m             mmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mmap,\n\u001b[1;32m    398\u001b[0m         )\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_fileobj() \u001b[38;5;28;01mas\u001b[39;00m fileobj:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fileslice(\n\u001b[1;32m    401\u001b[0m         fileobj,\n\u001b[1;32m    402\u001b[0m         slicer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    407\u001b[0m         lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock,\n\u001b[1;32m    408\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nibabel/volumeutils.py:467\u001b[0m, in \u001b[0;36marray_from_file\u001b[0;34m(shape, in_dtype, infile, offset, order, mmap)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(infile, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreadinto\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    466\u001b[0m     data_bytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m(n_bytes)\n\u001b[0;32m--> 467\u001b[0m     n_read \u001b[38;5;241m=\u001b[39m infile\u001b[38;5;241m.\u001b[39mreadinto(data_bytes)\n\u001b[1;32m    468\u001b[0m     needs_copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/gzip.py:324\u001b[0m, in \u001b[0;36mGzipFile.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01merrno\u001b[39;00m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(errno\u001b[38;5;241m.\u001b[39mEBADF, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread() on write-only GzipFile object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer\u001b[38;5;241m.\u001b[39mread(size)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/_compression.py:68\u001b[0m, in \u001b[0;36mDecompressReader.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreadinto\u001b[39m(\u001b[38;5;28mself\u001b[39m, b):\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b) \u001b[38;5;28;01mas\u001b[39;00m view, view\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m byte_view:\n\u001b[0;32m---> 68\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m(byte_view))\n\u001b[1;32m     69\u001b[0m         byte_view[:\u001b[38;5;28mlen\u001b[39m(data)] \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/gzip.py:535\u001b[0m, in \u001b[0;36m_GzipReader.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39mneeds_input:\n\u001b[1;32m    534\u001b[0m     buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread(READ_BUFFER_SIZE)\n\u001b[0;32m--> 535\u001b[0m     uncompress \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39mdecompress(buf, size)\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    537\u001b[0m     uncompress \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39mdecompress(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# 6. CALL FUNCTION FOR EACH SUBJECT\n",
    "# ----------------------------------------------------------------------\n",
    "for trait_label, trait_save in zip(ALL_TRAIT_LABELS, ALL_TRAIT_SAVE_STRS):\n",
    "\n",
    "    print(f\"\\n── TRAIT: {trait_label} ──\")\n",
    "\n",
    "    # 1) load that trait's behavioural RDM once\n",
    "    model_rdm = np.load(\n",
    "        os.path.join(deriv_dir, \"RDMs_behavior\",\n",
    "                     f\"{STIMULUS_LABEL_SAVE_STRING}_{trait_save}_RDM.npy\")\n",
    "    )\n",
    "\n",
    "    # 2) run every subject for this trait\n",
    "    for sub in subjects:\n",
    "        run_parcel_rsa_for_subject(sub, trait_save, model_rdm)\n",
    "\n",
    "        \n",
    "print(\"\\nALL DONE 🎉\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Analysis for Single Trait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 0) helper: run parcel-wise t-tests for ONE trait and save summary\n",
    "# ------------------------------------------------------------------\n",
    "def run_group_stats_for_trait(stim_label: str,\n",
    "                              trait_save: str,\n",
    "                              trait_long: str,\n",
    "                              subjects: list[str],\n",
    "                              n_rois: int):\n",
    "\n",
    "    # (1) gather all subject files for this trait\n",
    "    dfs = []\n",
    "    for sub in subjects:\n",
    "        filepath = os.path.join(\n",
    "            deriv_dir, \"RSA_stats\", stim_label, trait_save,\n",
    "            f\"{sub}_{stim_label}_{trait_save}_parcel_RSA_NNLS.csv\"   \n",
    "        )\n",
    "        dfs.append(pd.read_csv(filepath))\n",
    "    all_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "\n",
    "    # (2) sanity check\n",
    "    n_sub = all_df[\"subject\"].nunique()\n",
    "    print(f\"{trait_long}: concatenated {n_sub} subjects\")\n",
    "\n",
    "    # (3) parcel-wise one-sample t-tests\n",
    "    t_vals, p_vals = [], []\n",
    "    for parcel in range(1, n_rois + 1):\n",
    "        parcel_df = all_df[all_df[\"parcel_num\"] == parcel]\n",
    "        assert len(parcel_df) == n_sub, \\\n",
    "            f\"Parcel {parcel}: expected {n_sub} rows, got {len(parcel_df)}\"\n",
    "        t, p = ttest_1samp(parcel_df[\"beta\"], 0, alternative=\"greater\")\n",
    "        t_vals.append(t); p_vals.append(p)\n",
    "\n",
    "    # (4) summary dataframe\n",
    "    summary = pd.DataFrame({\n",
    "        \"parcel_num\" : np.arange(1, n_rois + 1),\n",
    "        \"parcel_label\": labels[1:],       # skip background\n",
    "        \"t_value\"    : t_vals,\n",
    "        \"p_value\"    : p_vals\n",
    "    })\n",
    "    summary = summary.sort_values(\"parcel_num\")      # ← NEW\n",
    "    summary[\"parcel_label\"] = labels[1:]    # ← NEW\n",
    "    \n",
    "    # (5) save\n",
    "    out_dir = os.path.join(\n",
    "        deriv_dir, \"RSA_stats\", stim_label, trait_save\n",
    "    )\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    out_csv = os.path.join(\n",
    "        out_dir, f\"group_stats_{stim_label}_{trait_save}_NNLS.csv\"\n",
    "    )\n",
    "    summary.to_csv(out_csv, index=False)\n",
    "    print(f\"✅ Saved group stats for {trait_long} → {out_csv}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open-minded: concatenated 75 subjects\n",
      "✅ Saved group stats for Open-minded → /Volumes/Passport/fmriprep/derivatives/RSA_stats/pieman/Open_minded/group_stats_pieman_Open_minded_NNLS.csv\n",
      "\n",
      "feeling Affectionate: concatenated 75 subjects\n",
      "✅ Saved group stats for feeling Affectionate → /Volumes/Passport/fmriprep/derivatives/RSA_stats/pieman/feeling_Affectionate/group_stats_pieman_feeling_Affectionate_NNLS.csv\n",
      "\n",
      "Attentive: concatenated 75 subjects\n",
      "✅ Saved group stats for Attentive → /Volumes/Passport/fmriprep/derivatives/RSA_stats/pieman/Attentive/group_stats_pieman_Attentive_NNLS.csv\n",
      "\n",
      "Assertive: concatenated 75 subjects\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# 1) DRIVER LOOP  (iterate over all 13 traits)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m trait_long, trait_save \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(ALL_TRAIT_LABELS, ALL_TRAIT_SAVE_STRS):\n\u001b[0;32m----> 5\u001b[0m     run_group_stats_for_trait(\n\u001b[1;32m      6\u001b[0m         STIMULUS_LABEL_SAVE_STRING,\n\u001b[1;32m      7\u001b[0m         trait_save,\n\u001b[1;32m      8\u001b[0m         trait_long,\n\u001b[1;32m      9\u001b[0m         subjects,\n\u001b[1;32m     10\u001b[0m         n_rois\n\u001b[1;32m     11\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[10], line 31\u001b[0m, in \u001b[0;36mrun_group_stats_for_trait\u001b[0;34m(stim_label, trait_save, trait_long, subjects, n_rois)\u001b[0m\n\u001b[1;32m     28\u001b[0m     parcel_df \u001b[38;5;241m=\u001b[39m all_df[all_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparcel_num\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m parcel]\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(parcel_df) \u001b[38;5;241m==\u001b[39m n_sub, \\\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParcel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparcel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_sub\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m rows, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(parcel_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 31\u001b[0m     t, p \u001b[38;5;241m=\u001b[39m ttest_1samp(parcel_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeta\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;241m0\u001b[39m, alternative\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgreater\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m     t_vals\u001b[38;5;241m.\u001b[39mappend(t); p_vals\u001b[38;5;241m.\u001b[39mappend(p)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# (4) summary dataframe\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531\u001b[0m, in \u001b[0;36m_axis_nan_policy_factory.<locals>.axis_nan_policy_decorator.<locals>.axis_nan_policy_wrapper\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sentinel:\n\u001b[1;32m    530\u001b[0m     samples \u001b[38;5;241m=\u001b[39m _remove_sentinel(samples, paired, sentinel)\n\u001b[0;32m--> 531\u001b[0m res \u001b[38;5;241m=\u001b[39m hypotest_fun_out(\u001b[38;5;241m*\u001b[39msamples, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    532\u001b[0m res \u001b[38;5;241m=\u001b[39m result_to_tuple(res)\n\u001b[1;32m    533\u001b[0m res \u001b[38;5;241m=\u001b[39m _add_reduced_axes(res, reduced_axes, keepdims)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/scipy/stats/_stats_py.py:6883\u001b[0m, in \u001b[0;36mttest_1samp\u001b[0;34m(a, popmean, axis, nan_policy, alternative)\u001b[0m\n\u001b[1;32m   6881\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(divide\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m, invalid\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m   6882\u001b[0m     t \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdivide(d, denom)[()]\n\u001b[0;32m-> 6883\u001b[0m prob \u001b[38;5;241m=\u001b[39m _get_pvalue(t, distributions\u001b[38;5;241m.\u001b[39mt(df), alternative)\n\u001b[1;32m   6885\u001b[0m \u001b[38;5;66;03m# when nan_policy='omit', `df` can be different for different axis-slices\u001b[39;00m\n\u001b[1;32m   6886\u001b[0m df \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mbroadcast_to(df, t\u001b[38;5;241m.\u001b[39mshape)[()]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/scipy/stats/_distn_infrastructure.py:851\u001b[0m, in \u001b[0;36mrv_generic.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m--> 851\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfreeze(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/scipy/stats/_distn_infrastructure.py:846\u001b[0m, in \u001b[0;36mrv_generic.freeze\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Freeze the distribution for the given arguments.\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \n\u001b[1;32m    833\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    843\u001b[0m \n\u001b[1;32m    844\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    845\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, rv_continuous):\n\u001b[0;32m--> 846\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rv_continuous_frozen(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    847\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    848\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rv_discrete_frozen(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/scipy/stats/_distn_infrastructure.py:465\u001b[0m, in \u001b[0;36mrv_frozen.__init__\u001b[0;34m(self, dist, *args, **kwds)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds \u001b[38;5;241m=\u001b[39m kwds\n\u001b[1;32m    464\u001b[0m \u001b[38;5;66;03m# create a new instance\u001b[39;00m\n\u001b[0;32m--> 465\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdist \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdist\u001b[38;5;241m.\u001b[39m_updated_ctor_param())\n\u001b[1;32m    467\u001b[0m shapes, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdist\u001b[38;5;241m.\u001b[39m_parse_args(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdist\u001b[38;5;241m.\u001b[39m_get_support(\u001b[38;5;241m*\u001b[39mshapes)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/scipy/stats/_distn_infrastructure.py:1850\u001b[0m, in \u001b[0;36mrv_continuous.__init__\u001b[0;34m(self, momtype, a, b, xtol, badvalue, name, longname, shapes, seed)\u001b[0m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1849\u001b[0m     dct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(distcont)\n\u001b[0;32m-> 1850\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_doc(docdict, dct\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/scipy/stats/_distn_infrastructure.py:812\u001b[0m, in \u001b[0;36mrv_generic._construct_doc\u001b[0;34m(self, docdict, shapes_vals)\u001b[0m\n\u001b[1;32m    810\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%(shapes)s\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    811\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 812\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m \u001b[38;5;241m=\u001b[39m doccer\u001b[38;5;241m.\u001b[39mdocformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m, tempdict)\n\u001b[1;32m    813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    814\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to construct docstring for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    815\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistribution \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/scipy/_lib/doccer.py:61\u001b[0m, in \u001b[0;36mdocformat\u001b[0;34m(docstring, docdict)\u001b[0m\n\u001b[1;32m     59\u001b[0m indented \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, dstr \u001b[38;5;129;01min\u001b[39;00m docdict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 61\u001b[0m     lines \u001b[38;5;241m=\u001b[39m dstr\u001b[38;5;241m.\u001b[39mexpandtabs()\u001b[38;5;241m.\u001b[39msplitlines()\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     63\u001b[0m         newlines \u001b[38;5;241m=\u001b[39m [lines[\u001b[38;5;241m0\u001b[39m]]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 1) DRIVER LOOP  (iterate over all 13 traits)\n",
    "# ------------------------------------------------------------------\n",
    "for trait_long, trait_save in zip(ALL_TRAIT_LABELS, ALL_TRAIT_SAVE_STRS):\n",
    "    run_group_stats_for_trait(\n",
    "        STIMULUS_LABEL_SAVE_STRING,\n",
    "        trait_save,\n",
    "        trait_long,\n",
    "        subjects,\n",
    "        n_rois\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
