{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NNLS Permutation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements full NNLS + permutation-testing pipeline:\n",
    "\n",
    "1. **Observed s₁**: highest beta per parcel per subject, averaged across subjects.\n",
    "2. **Null distribution**: circularly shift each 160×160 trait RDM by a random TR, rerun NNLS, extract s₁, average → repeat `n_perm` times.\n",
    "3. **Permutation p-value**: compare observed s₁ to null distribution for each parcel.\n",
    "4. **Save** results to a dedicated output folder under `/Volumes/Passport/fmriprep/derivatives/RSA_stats/pieman`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from scipy.optimize import nnls\n",
    "import os, glob, re, csv\n",
    "import glob\n",
    "import nibabel as nib\n",
    "import statsmodels.api as sm\n",
    "from nilearn import datasets\n",
    "from nilearn.image import resample_to_img\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_1samp\n",
    "from nilearn import plotting, datasets\n",
    "import scipy.stats as stats\n",
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Cleaned BOLD Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voxel size (mm): (3.0, 3.0, 4.0)\n",
      "TR (s): 1.0\n",
      "Shape (65, 77, 49, 160)\n"
     ]
    }
   ],
   "source": [
    "# Load the BOLD cleaned image\n",
    "bold_img = nib.load('/Volumes/Passport/fmriprep/derivatives/pieMan_cleaned/sub-002/func/sub-002_task-pieman_run-1_cleaned_desc-masked_bold.nii.gz')\n",
    "\n",
    "# Print voxel size (spatial resolution) and TR (temporal resolution)\n",
    "zooms = bold_img.header.get_zooms()\n",
    "print(f\"Voxel size (mm): {zooms[:3]}\")\n",
    "print(f\"TR (s): {zooms[3]}\")\n",
    "print(f\"Shape {bold_img.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Set Parameters/Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET MAIN HYPERPARAMETERS\n",
    "# TRAIT_LABEL = \"Contemplating\"  \n",
    "\n",
    "TRAIT_SETS = {\n",
    "    \"all_13\": [\n",
    "        \"Open-minded\",\"feeling Affectionate\",\"Attentive\",\"Assertive\",\n",
    "        \"feeling Gloomy\",\"feeling Peaceful\",\"Agreeable\",\"Judging\",\n",
    "        \"feeling Angry\",\"feeling Bewildered\",\"Impulsive\",\n",
    "        \"Self-disciplined\",\"Contemplating\"\n",
    "    ],\n",
    "    \"mental_8\": [\n",
    "        \"feeling Affectionate\",\"feeling Gloomy\",\"feeling Peaceful\",\n",
    "        \"feeling Angry\",\"feeling Bewildered\",\"Judging\",\n",
    "        \"Contemplating\",\"Attentive\"\n",
    "    ],\n",
    "    \"personality_5\": [\n",
    "        \"Open-minded\",\"Agreeable\",\"Assertive\",\n",
    "        \"Self-disciplined\",\"Impulsive\"\n",
    "    ],\n",
    "    \"trait_9\": [\n",
    "        \"Open-minded\",\"feeling Affectionate\",\"Attentive\",\"Assertive\",\n",
    "        \"Agreeable\",\"Judging\",\"feeling Angry\",\"Self-disciplined\",\"Contemplating\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Select model here: choose one key from TRAIT_SETS\n",
    "model_key = \"trait_9\"   # options: all_13, mental_8, personality_5, drop4_9\n",
    "traits    = TRAIT_SETS[model_key]\n",
    "\n",
    "# ALL_TRAIT_LABELS = [\n",
    "    #\"Open-minded\",\"feeling Affectionate\",\"Attentive\",\"Assertive\",\n",
    "    #\"feeling Gloomy\",\"feeling Peaceful\",\"Agreeable\",\"Judging\",\n",
    "    #\"feeling Angry\",\"feeling Bewildered\",\"Impulsive\",\n",
    "    #\"Self-disciplined\",\"Contemplating\"\n",
    "#]\n",
    "ALL_TRAIT_SAVE_STRS = [t.replace(\" \",\"_\").replace(\"-\",\"_\")\n",
    "                       for t in traits]\n",
    "# Our 13 trait labels \n",
    "# [\"Open-minded\", \"feeling Affectionate\", \"Attentive\", \"Assertive\", \"feeling Gloomy\", \"feeling Peaceful\", \"Agreeable\", \"Judging\", \"feeling Angry\", \"feeling Bewildered\", \"Impulsive\", \"Self-disciplined\", \"Contemplating\"]\n",
    "\n",
    "#TRAIT_LABEL_SAVE_STRING = TRAIT_LABEL.replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "STIMULUS_LABEL_SAVE_STRING = \"pieman\"\n",
    "\n",
    "# Set window shift amount\n",
    "shift_window = 20  #shifts the window by x TRs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────\n",
    "# 0) PATHS & I/O\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "root_dir  = \"/Volumes/Passport/fmriprep\"          # ←  same as in cleaning script\n",
    "deriv_dir = os.path.join(root_dir, \"derivatives\") #   (don’t hard-code “subjects” yet)\n",
    "\n",
    "\n",
    "\n",
    "# output from your behaviour-model RSA\n",
    "for trait_long, trait_save in zip(traits, ALL_TRAIT_SAVE_STRS):\n",
    "    rdm_path = os.path.join(\n",
    "        deriv_dir, \"RDMs_behavior\",\n",
    "        f\"{STIMULUS_LABEL_SAVE_STRING}_{trait_save}_RDM.npy\"\n",
    "    )\n",
    "    model_rdm = np.load(rdm_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Filter Subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────\n",
    "# 1) SUBJECT / RUN FILTERS  (copy-paste verbatim)  ─────────────\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "exclude_subs = {\n",
    "    \"sub-001\",\"sub-021\",\"sub-022\",\"sub-038\",\"sub-056\",\"sub-068\",\"sub-069\"\n",
    "}\n",
    "exclude_sub_runs = {\n",
    "    (\"sub-002\",\"2\"),(\"sub-003\",\"2\"),(\"sub-004\",\"2\"),(\"sub-005\",\"2\"),(\"sub-006\",\"2\"),\n",
    "    (\"sub-008\",\"2\"),(\"sub-010\",\"2\"),(\"sub-011\",\"2\"),(\"sub-012\",\"2\"),(\"sub-013\",\"2\"),\n",
    "    (\"sub-014\",\"2\"),(\"sub-015\",\"2\"),(\"sub-016\",\"2\")\n",
    "}\n",
    "target_subject = None     # e.g. \"sub-002\" to run a single person\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subjects to process → sub-002, sub-003, sub-004, sub-005, sub-006, sub-007, sub-008, sub-009, sub-010, sub-011, sub-012, sub-013, sub-014, sub-015, sub-016, sub-017, sub-018, sub-019, sub-020, sub-023, sub-024, sub-025, sub-026, sub-027, sub-028, sub-029, sub-030, sub-031, sub-032, sub-033, sub-034, sub-035, sub-036, sub-037, sub-039, sub-040, sub-041, sub-042, sub-043, sub-044, sub-045, sub-046, sub-047, sub-048, sub-049, sub-050, sub-051, sub-052, sub-053, sub-054, sub-055, sub-057, sub-058, sub-059, sub-060, sub-061, sub-062, sub-063, sub-064, sub-065, sub-066, sub-067, sub-070, sub-071, sub-072, sub-073, sub-074, sub-075, sub-076, sub-077, sub-078, sub-079, sub-080, sub-081, sub-082\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# 2)  BUILD SUBJECT LIST  (from cleaned derivatives)  ------------\n",
    "# ----------------------------------------------------------------\n",
    "cleaned_root = os.path.join(deriv_dir, f\"{STIMULUS_LABEL_SAVE_STRING}_cleaned\")\n",
    "all_subs     = sorted(\n",
    "    d for d in os.listdir(cleaned_root) if d.startswith(\"sub-\")\n",
    ")\n",
    "if target_subject:\n",
    "    if target_subject not in all_subs:\n",
    "        raise ValueError(f\"{target_subject} not found in {cleaned_root}\")\n",
    "    subjects = [target_subject]\n",
    "else:\n",
    "    subjects = [s for s in all_subs if s not in exclude_subs]\n",
    "\n",
    "print(\"Subjects to process →\", \", \".join(subjects))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Fetch Schaefer Atlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────\n",
    "# FETCH SCHAEFER ATLAS  ─────────────────────────────────────\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "\n",
    "# Schaefer parcel/atlas parameters\n",
    "n_rois = 200\n",
    "yeo_networks = 17\n",
    "resolution_mm = 2                   # resolution of your Schaefer atlas (double check!)\n",
    "\n",
    "schaefer    = datasets.fetch_atlas_schaefer_2018(\n",
    "                 n_rois=n_rois,\n",
    "                 yeo_networks=yeo_networks,\n",
    "                 resolution_mm=resolution_mm\n",
    "             )\n",
    "atlas_img   = nib.load(schaefer['maps'])  # default 2mm MNI - but our images 3x3x4 (Pieman and others) OR 2.5^3 (ie., Black and Forgot)\n",
    "\n",
    "atlas_resampled = resample_to_img(atlas_img, bold_img, interpolation='nearest')\n",
    "atlas_data     = atlas_resampled.get_fdata()\n",
    "\n",
    "\n",
    "\n",
    "# Change Schaeffer Labels so 0 is whole brain and 1 corresponds to 1st ROI\n",
    "labels = schaefer['labels']\n",
    "# change to string and remove excess\n",
    "labels = [l.replace(b'17Networks_', b'').decode('utf-8') for l in labels]\n",
    "# Prepend background label\n",
    "labels = np.insert(labels, 0, \"Background\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Permutations: 100%|██████████| 100/100 [23:24<00:00, 14.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Permutation results for k=1…9 saved → /Volumes/Passport/fmriprep/derivatives/RSA_stats/pieman/permutation_testing/perm_test_trait_9_allks.csv\n"
     ]
    }
   ],
   "source": [
    "# ─── PERMUTATION TESTING Pipeline (s₁…sₙ) ────────────────────────────────\n",
    "from scipy.optimize import nnls\n",
    "from tqdm import tqdm\n",
    "\n",
    "def vectorize_rdm(rdm):\n",
    "    idx = np.tril_indices(rdm.shape[0], k=-1)\n",
    "    return rdm[idx]\n",
    "\n",
    "# 9.1) load all subject‐run CSVs with betas (unchanged)\n",
    "multi_csvs = glob.glob(os.path.join(\n",
    "    deriv_dir, \"RSA_stats\", STIMULUS_LABEL_SAVE_STRING,\n",
    "    \"multi_regression\",\n",
    "    f\"*_{STIMULUS_LABEL_SAVE_STRING}_multi_parcel_RSA_NNLS_{model_key}.csv\"\n",
    "))\n",
    "\n",
    "# assert we have one file per subject (add dictionary for different stimuli later)\n",
    "assert len(multi_csvs) == 75, (                     \n",
    "    f\"Expected 75 files, but found {len(multi_csvs)}\"\n",
    ")\n",
    "\n",
    "df_list = []\n",
    "for fn in multi_csvs:\n",
    "    df = pd.read_csv(fn)\n",
    "    df['unit'] = df['subject'] + \"_\" + df['run'].astype(str)\n",
    "    df_list.append(df)\n",
    "all_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "trait_cols = ALL_TRAIT_SAVE_STRS\n",
    "n_traits  = len(trait_cols)\n",
    "\n",
    "# 9.2) compute observed s_k per parcel for k=1…n_traits\n",
    "observed = {}\n",
    "# sort-and-sum top-k per row, then average by parcel\n",
    "for k in range(1, n_traits+1):\n",
    "    # descending sort, sum top k\n",
    "    all_df[f's{k}'] = (\n",
    "        np.sort(all_df[trait_cols].values, axis=1) # 1) sort each row ascending\n",
    "        [:, ::-1]   # 2) reverse to descending \n",
    "        [:, :k]      # 3) take top-k\n",
    "        .sum(axis=1)  # 4) sum across rows\n",
    "    )\n",
    "    observed[k] = all_df.groupby('parcel_num')[f's{k}'].mean()\n",
    "\n",
    "# 9.3) precompute & cache every neural RDM (unchanged)\n",
    "neural_rdm_cache = {}\n",
    "for sub in subjects:\n",
    "    func_dir = os.path.join(cleaned_root, sub, \"func\")\n",
    "    run_pat  = os.path.join(func_dir,\n",
    "        f\"{sub}_task-{STIMULUS_LABEL_SAVE_STRING}_run-*_*cleaned_desc-masked_bold.nii.gz\"\n",
    "    )\n",
    "    single_pat = os.path.join(func_dir,\n",
    "        f\"{sub}_task-{STIMULUS_LABEL_SAVE_STRING}_cleaned_desc-masked_bold.nii.gz\"\n",
    "    )\n",
    "    bold_files = sorted(glob.glob(run_pat)) + sorted(glob.glob(single_pat))\n",
    "    for bf in bold_files:\n",
    "        m = re.search(r\"_run-(\\d+)_\", os.path.basename(bf))\n",
    "        run = m.group(1) if m else \"NA\"\n",
    "        if (sub, run) in exclude_sub_runs: \n",
    "            continue\n",
    "        img = nib.load(bf)\n",
    "        atlas_res = resample_to_img(atlas_img, img, interpolation='nearest')\n",
    "        atlas_dat = atlas_res.get_fdata().astype(int)\n",
    "        bold_dat  = img.get_fdata()\n",
    "        for pid in range(1, n_rois+1):\n",
    "            mask = atlas_dat == pid\n",
    "            if not mask.any(): \n",
    "                continue\n",
    "            # 1 - corr → RDM\n",
    "            corr = np.corrcoef(bold_dat[mask,:].T)\n",
    "            neural_rdm_cache[(sub, run, pid)] = (1 - corr).astype(np.float32)\n",
    "\n",
    "    \n",
    "\n",
    "# 9.4) load behavior RDMs (unchanged)\n",
    "behavior_rdms = {\n",
    "    trait: np.load(os.path.join(\n",
    "        deriv_dir, \"RDMs_behavior\",\n",
    "        f\"{STIMULUS_LABEL_SAVE_STRING}_{sstr}_RDM.npy\"\n",
    "    ))\n",
    "    for trait, sstr in zip(traits, ALL_TRAIT_SAVE_STRS)\n",
    "}\n",
    "\n",
    "# 9.5) build null distributions for sₖ\n",
    "n_perm = 100\n",
    "parcel_ids = list(observed[1].index)\n",
    "nulls = {k: {pid: [] for pid in parcel_ids} for k in range(1, n_traits+1)}\n",
    "n_tr   = next(iter(behavior_rdms.values())).shape[0]\n",
    "\n",
    "assert n_tr == 160, f\"Expected 160 timepoints, got {n_tr}\"\n",
    "\n",
    "for i in tqdm(range(n_perm), desc=\"Permutations\"):\n",
    "    # random circular TR shift ≥20\n",
    "    shift = np.random.randint(shift_window, n_tr-shift_window)\n",
    "    permuted = {\n",
    "        trait: vectorize_rdm(np.roll(np.roll(mat, shift, axis=0),\n",
    "                                     shift, axis=1))\n",
    "        for trait, mat in behavior_rdms.items()\n",
    "    }\n",
    "\n",
    "    # accumulate sums & counts for each k & parcel\n",
    "    sums   = {k: {pid: 0.0 for pid in parcel_ids} for k in range(1, n_traits+1)}\n",
    "    counts = {k: {pid: 0   for pid in parcel_ids} for k in range(1, n_traits+1)}\n",
    "\n",
    "    for (sub, run, pid), nr in neural_rdm_cache.items():\n",
    "        y = vectorize_rdm(nr)\n",
    "        X = np.column_stack([np.ones_like(y)] +\n",
    "                             [permuted[t] for t in traits])\n",
    "        coef, _    = nnls(X, y)\n",
    "        betas      = coef[1:]                     # drop intercept\n",
    "        top_sorted = np.sort(betas)[::-1]         # descending\n",
    "\n",
    "        # for each k, sum top-k betas\n",
    "        for k in range(1, n_traits+1):\n",
    "            s_k = top_sorted[:k].sum()\n",
    "            sums[k][pid]   += s_k\n",
    "            counts[k][pid] += 1\n",
    "\n",
    "    # record mean null s_k for each parcel\n",
    "    for k in range(1, n_traits+1):\n",
    "        for pid in parcel_ids:\n",
    "            nulls[k][pid].append(sums[k][pid] / counts[k][pid])\n",
    "\n",
    "# 9.6) compare observed vs null → p-values for every (k, parcel)\n",
    "results = []\n",
    "for k in range(1, n_traits+1):\n",
    "    for pid, obs_val in observed[k].items():\n",
    "        dist = np.array(nulls[k][pid])\n",
    "        pval = (np.sum(dist >= obs_val) + 1) / (n_perm + 1)\n",
    "        results.append((k, pid, obs_val, pval, pval < .05))\n",
    "\n",
    "df_perm = pd.DataFrame(\n",
    "    results,\n",
    "    columns=['k', 'parcel_num', 'observed_s', 'p_value', 'significant_p05']\n",
    ")\n",
    "\n",
    "# 9.7) add parcel labels and save\n",
    "parcel_labels = pd.DataFrame({\n",
    "    'parcel_num': np.arange(1, len(labels)),\n",
    "    'parcel_label': labels[1:]\n",
    "})\n",
    "# ─── define & create output folder ─────────────────────────────────────────\n",
    "perm_dir = os.path.join(\n",
    "    deriv_dir, \"RSA_stats\", STIMULUS_LABEL_SAVE_STRING, \"permutation_testing\"\n",
    ")\n",
    "os.makedirs(perm_dir, exist_ok=True)\n",
    "\n",
    "df_perm = df_perm.merge(parcel_labels, on='parcel_num')\n",
    "out_csv = os.path.join(perm_dir, f\"perm_test_{model_key}_allks.csv\")\n",
    "df_perm.to_csv(out_csv, index=False)\n",
    "print(f\"✅ Permutation results for k=1…{n_traits} saved → {out_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   1         2         3         4         5         6  \\\n",
      "parcel_num                                                               \n",
      "1           0.003325  0.005202  0.005988  0.006010  0.006010  0.006010   \n",
      "2           0.003341  0.005264  0.006161  0.006225  0.006225  0.006225   \n",
      "3           0.003271  0.005080  0.005815  0.005819  0.005819  0.005819   \n",
      "4           0.003182  0.005020  0.005743  0.005815  0.005830  0.005830   \n",
      "5           0.003276  0.005003  0.005746  0.005748  0.005748  0.005748   \n",
      "...              ...       ...       ...       ...       ...       ...   \n",
      "196         0.003260  0.004878  0.005529  0.005529  0.005529  0.005529   \n",
      "197         0.003168  0.004822  0.005446  0.005446  0.005446  0.005446   \n",
      "198         0.003290  0.005027  0.005653  0.005653  0.005653  0.005653   \n",
      "199         0.003550  0.005250  0.005929  0.005929  0.005929  0.005929   \n",
      "200         0.003331  0.004992  0.005682  0.005682  0.005682  0.005682   \n",
      "\n",
      "                   7         8         9  \n",
      "parcel_num                                \n",
      "1           0.006010  0.006010  0.006010  \n",
      "2           0.006225  0.006225  0.006225  \n",
      "3           0.005819  0.005819  0.005819  \n",
      "4           0.005830  0.005830  0.005830  \n",
      "5           0.005748  0.005748  0.005748  \n",
      "...              ...       ...       ...  \n",
      "196         0.005529  0.005529  0.005529  \n",
      "197         0.005446  0.005446  0.005446  \n",
      "198         0.005653  0.005653  0.005653  \n",
      "199         0.005929  0.005929  0.005929  \n",
      "200         0.005682  0.005682  0.005682  \n",
      "\n",
      "[200 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "observed_df = pd.DataFrame(observed)\n",
    "print(observed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1800 entries, 0 to 1799\n",
      "Data columns (total 6 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   k                1800 non-null   int64  \n",
      " 1   parcel_num       1800 non-null   int64  \n",
      " 2   observed_s       1800 non-null   float64\n",
      " 3   p_value          1800 non-null   float64\n",
      " 4   significant_p05  1800 non-null   bool   \n",
      " 5   parcel_label     1800 non-null   object \n",
      "dtypes: bool(1), float64(2), int64(2), object(1)\n",
      "memory usage: 72.2+ KB\n",
      "None\n",
      "   k  parcel_num  observed_s   p_value  significant_p05          parcel_label\n",
      "0  1           1    0.003325  0.851485            False    LH_VisCent_ExStr_1\n",
      "1  1           2    0.003341  1.000000            False    LH_VisCent_ExStr_2\n",
      "2  1           3    0.003271  0.960396            False  LH_VisCent_Striate_1\n",
      "3  1           4    0.003182  1.000000            False    LH_VisCent_ExStr_3\n",
      "4  1           5    0.003276  0.792079            False    LH_VisCent_ExStr_4\n"
     ]
    }
   ],
   "source": [
    "df_perm = pd.read_csv(out_csv)\n",
    "print(df_perm.info())\n",
    "print(df_perm.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Whole Brain Parcellation Map of p-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ thresholded k=1 p-value map saved → /Volumes/Passport/fmriprep/derivatives/RSA_stats/pieman/permutation_testing/perm_pmap_trait_9_k1_thresh05.nii.gz\n"
     ]
    }
   ],
   "source": [
    "# Filter to only the k=1 results\n",
    "df_k1 = df_perm[df_perm['k'] == 1]\n",
    "\n",
    "# 1) Extract the integer parcel map\n",
    "atlas_data = atlas_resampled.get_fdata().astype(int)\n",
    "\n",
    "# 2) Create a blank p-map\n",
    "p_map = np.zeros(atlas_data.shape, dtype=float)\n",
    "\n",
    "# 3) Fill in each parcel with its k=1 p-value\n",
    "for _, row in df_k1.iterrows():\n",
    "    pid  = int(row['parcel_num'])\n",
    "    pval = float(row['p_value'])\n",
    "    p_map[atlas_data == pid] = pval\n",
    "\n",
    "# 4) Threshold at p < .05: zero out all non-significant voxels\n",
    "p_map[p_map >= 0.05] = 0.0\n",
    "\n",
    "# 5) Wrap it in a Nifti and save\n",
    "p_img   = nib.Nifti1Image(p_map, atlas_resampled.affine, atlas_resampled.header)\n",
    "out_nii = os.path.join(perm_dir, f\"perm_pmap_{model_key}_k1_thresh05.nii.gz\")\n",
    "nib.save(p_img, out_nii)\n",
    "print(f\"✅ thresholded k=1 p-value map saved → {out_nii}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
